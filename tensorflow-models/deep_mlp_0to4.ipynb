{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is based on:\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import graph_util as gu\n",
    "from tensorflow.python.framework.graph_util import remove_training_nodes\n",
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-08601696ce79>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/yuezha01/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"mnist_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train = mnist.train.images\n",
    "labels_train = mnist.train.labels\n",
    "cls_train = np.matmul(labels_train, np.arange(10))\n",
    "cls_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = cls_train.argsort()\n",
    "cls_train_sorted = cls_train[order]\n",
    "images_train_sorted = images_train[order]\n",
    "labels_train_sorted = labels_train[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distribution of digits is uneven. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    6179\n",
       "7.0    5715\n",
       "3.0    5638\n",
       "2.0    5470\n",
       "9.0    5454\n",
       "0.0    5444\n",
       "6.0    5417\n",
       "8.0    5389\n",
       "4.0    5307\n",
       "5.0    4987\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The distribution of digits is uneven. \")\n",
    "pd.Series(cls_train_sorted).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28038 numbers from 0 to 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    6179\n",
       "3.0    5638\n",
       "2.0    5470\n",
       "0.0    5444\n",
       "4.0    5307\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_df = pd.Series(cls_train_sorted)\n",
    "cls_df = cls_df.loc[cls_df < 5]\n",
    "count_0to4 = cls_df.shape[0]\n",
    "print(\"{} numbers from 0 to 4\".format(count_0to4))\n",
    "cls_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28038, 784)\n",
      "(28038, 10)\n",
      "(28038,)\n"
     ]
    }
   ],
   "source": [
    "images_train_sorted_0to4 = images_train_sorted[0:count_0to4, :]\n",
    "labels_train_sorted_0to4 = labels_train_sorted[0:count_0to4, :]\n",
    "cls_train_sorted_0to4 = cls_train_sorted[0:count_0to4]\n",
    "print(images_train_sorted_0to4.shape)\n",
    "print(labels_train_sorted_0to4.shape)\n",
    "print(cls_train_sorted_0to4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n"
     ]
    }
   ],
   "source": [
    "print(\"Shuffle data\")\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(images_train_sorted_0to4)\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(labels_train_sorted_0to4)\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(cls_train_sorted_0to4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the three arrays match:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[4. 3. 3. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Check the three arrays match:\")\n",
    "print(labels_train_sorted_0to4[[10, 101, 10001, 20000], :])\n",
    "print(cls_train_sorted_0to4[[10, 101, 10001, 20000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_val = mnist.validation.images\n",
    "labels_val = mnist.validation.labels\n",
    "cls_val = np.matmul(labels_val, np.arange(10))\n",
    "cls_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = cls_val.argsort()\n",
    "cls_val_sorted = cls_val[order]\n",
    "images_val_sorted = images_val[order]\n",
    "labels_val_sorted = labels_val[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distribution of digits is uneven. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    563\n",
       "7.0    550\n",
       "4.0    535\n",
       "6.0    501\n",
       "9.0    495\n",
       "3.0    493\n",
       "2.0    488\n",
       "0.0    479\n",
       "8.0    462\n",
       "5.0    434\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The distribution of digits is uneven. \")\n",
    "pd.Series(cls_val_sorted).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2558 numbers from 0 to 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    563\n",
       "4.0    535\n",
       "3.0    493\n",
       "2.0    488\n",
       "0.0    479\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_val_df = pd.Series(cls_val_sorted)\n",
    "cls_val_df = cls_val_df.loc[cls_val_df < 5]\n",
    "count_val_0to4 = cls_val_df.shape[0]\n",
    "print(\"{} numbers from 0 to 4\".format(count_val_0to4))\n",
    "cls_val_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2558, 784)\n",
      "(2558, 10)\n",
      "(2558,)\n"
     ]
    }
   ],
   "source": [
    "images_val_sorted_0to4 = images_val_sorted[0:count_val_0to4, :]\n",
    "labels_val_sorted_0to4 = labels_val_sorted[0:count_val_0to4, :]\n",
    "cls_val_sorted_0to4 = cls_val_sorted[0:count_val_0to4]\n",
    "print(images_val_sorted_0to4.shape)\n",
    "print(labels_val_sorted_0to4.shape)\n",
    "print(cls_val_sorted_0to4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data\n"
     ]
    }
   ],
   "source": [
    "print(\"Shuffle data\")\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(images_val_sorted_0to4)\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(labels_val_sorted_0to4)\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(cls_val_sorted_0to4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the three arrays match:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[4. 4. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Check the three arrays match:\")\n",
    "print(labels_val_sorted_0to4[[2, 331, 445, 2000], :])\n",
    "print(cls_val_sorted_0to4[[2, 331, 445, 2000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepnn(x):\n",
    "    W_fc1 = weight_variable([784, 128], name='W_fc1')\n",
    "    b_fc1 = bias_variable([128], name='b_fc1')\n",
    "    a_fc1 = tf.add(tf.matmul(x, W_fc1), b_fc1, name=\"zscore\")\n",
    "    h_fc1 = tf.nn.relu(a_fc1)\n",
    "    \n",
    "    W_fc2 = weight_variable([128, 64], name='W_fc2')\n",
    "    b_fc2 = bias_variable([64], name='b_fc2')\n",
    "    a_fc2 = tf.add(tf.matmul(h_fc1, W_fc2), b_fc2, name=\"zscore\")\n",
    "    h_fc2 = tf.nn.relu(a_fc2)\n",
    "    \n",
    "    W_fc3 = weight_variable([64, 10], name='W_fc3')\n",
    "    b_fc3 = bias_variable([10], name='b_fc3')\n",
    "    logits = tf.add(tf.matmul(h_fc2, W_fc3), b_fc3, name=\"logits\")\n",
    "    y_pred = tf.argmax(logits, 1, name='y_pred')\n",
    "\n",
    "    return y_pred, logits\n",
    "\n",
    "\n",
    "def weight_variable(shape, name):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name)\n",
    "\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x\")\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y\")\n",
    "\n",
    "# Build the graph for the deep net\n",
    "y_pred, logits = deepnn(x)\n",
    "\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, \n",
    "                                                               logits=logits)\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"cross_entropy_loss\")\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss, name=\"train_step\")\n",
    "  \n",
    "# Here we specify the output as \"Prediction/y_pred\", this will be important later\n",
    "with tf.name_scope(\"Prediction\"): \n",
    "    correct_prediction = tf.equal(y_pred, \n",
    "                                  tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "n_batches = int(images_train_sorted_0to4.shape[0] / batch_size)\n",
    "print(n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 2.5812, training accuracy: 0.126828\n",
      "Epoch 0, validation loss: 2.58384, validation accuracy 0.135262\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "train_loss, train_accuracy = sess.run([loss, accuracy], feed_dict={x: images_train_sorted_0to4, \n",
    "                                                                   y_: labels_train_sorted_0to4})\n",
    "print('Epoch %d, training loss: %g, training accuracy: %g' % (0, train_loss, train_accuracy))\n",
    "val_loss, val_accuracy = sess.run([loss, accuracy], feed_dict={x: images_val_sorted_0to4,\n",
    "                                                               y_: labels_val_sorted_0to4})\n",
    "print('Epoch %d, validation loss: %g, validation accuracy %g' % (0, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.172996, training accuracy: 0.957379\n",
      "Epoch 0, validation loss: 0.171218, validation accuracy 0.961689\n",
      "Epoch 10, training loss: 0.0385939, training accuracy: 0.989372\n",
      "Epoch 10, validation loss: 0.0499171, validation accuracy 0.983972\n",
      "Epoch 20, training loss: 0.0149445, training accuracy: 0.996469\n",
      "Epoch 20, validation loss: 0.0377949, validation accuracy 0.98749\n",
      "Epoch 30, training loss: 0.00547536, training accuracy: 0.999215\n",
      "Epoch 30, validation loss: 0.0383306, validation accuracy 0.987881\n",
      "Epoch 40, training loss: 0.001544, training accuracy: 0.999857\n",
      "Epoch 40, validation loss: 0.0419834, validation accuracy 0.989054\n",
      "Epoch 50, training loss: 0.000368855, training accuracy: 1\n",
      "Epoch 50, validation loss: 0.0503327, validation accuracy 0.989054\n",
      "Epoch 60, training loss: 8.60282e-05, training accuracy: 1\n",
      "Epoch 60, validation loss: 0.0598857, validation accuracy 0.990618\n",
      "Epoch 70, training loss: 2.14553e-05, training accuracy: 1\n",
      "Epoch 70, validation loss: 0.0709655, validation accuracy 0.989836\n",
      "Epoch 80, training loss: 5.44178e-06, training accuracy: 1\n",
      "Epoch 80, validation loss: 0.0825253, validation accuracy 0.989836\n",
      "Epoch 90, training loss: 1.21422e-06, training accuracy: 1\n",
      "Epoch 90, validation loss: 0.0932233, validation accuracy 0.990618\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "    batch_start = 0\n",
    "    batch_end = batch_start + batch_size\n",
    "    \n",
    "    for j in range(n_batches):\n",
    "        if j == n_batches - 1:\n",
    "            batch_images = images_train_sorted_0to4[batch_start:, :]\n",
    "            batch_labels = labels_train_sorted_0to4[batch_start:, :]\n",
    "        else:\n",
    "            batch_images = images_train_sorted_0to4[batch_start:batch_end, :]\n",
    "            batch_labels = labels_train_sorted_0to4[batch_start:batch_end, :]\n",
    "        batch_start += batch_size\n",
    "        batch_end += batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict={x: batch_images, y_: batch_labels})\n",
    "        \n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        train_loss, train_accuracy = sess.run([loss, accuracy], feed_dict={x: images_train_sorted_0to4, \n",
    "                                                                           y_: labels_train_sorted_0to4})\n",
    "        print('Epoch %d, training loss: %g, training accuracy: %g' % (i, train_loss, train_accuracy))\n",
    "        val_loss, val_accuracy = sess.run([loss, accuracy], feed_dict={x: images_val_sorted_0to4,\n",
    "                                                                       y_: labels_val_sorted_0to4})\n",
    "        print('Epoch %d, validation loss: %g, validation accuracy %g' % (i, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.51\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy %g' % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'W_fc1' has type str, but expected one of: int, long, bool\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'W_fc1' has type str, but expected one of: int, long, bool\n",
      "['y_pred']\n"
     ]
    }
   ],
   "source": [
    "saver.save(sess, \"./chkps/mnist_model_0to4\")\n",
    "out_nodes = [y_pred.op.name]\n",
    "print(out_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graph_def = remove_training_nodes(sess.graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 6 variables.\n",
      "INFO:tensorflow:Converted 6 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "sub_graph_def = gu.convert_variables_to_constants(sess, sub_graph_def, out_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written graph to: ./mnist_model_0to4/deep_mlp.pb\n"
     ]
    }
   ],
   "source": [
    "graph_path = tf.train.write_graph(sub_graph_def,\n",
    "                                  \"./mnist_model_0to4\",\n",
    "                                  \"deep_mlp.pb\",\n",
    "                                  as_text=False)\n",
    "\n",
    "print('written graph to: %s' % graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 3, 0, 3, 2, 4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(y_pred, feed_dict={x: mnist.test.images[[3, 2, 1, 18, 4, 15, 11, 0, 61, 7], :]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels[[3, 2, 1, 18, 4, 15, 11, 0, 61, 7], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = mnist.test.images[[3, 2, 1, 18, 4, 15, 11, 0, 61, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./images_0to9.csv\", images, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(images).to_csv(\"./images_0to9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./input_data.h\", \"w\") as hfile:\n",
    "    hfile.write(\"const float input_data [%s][%s] = {\\n\" % (images.shape[0], images.shape[1]))\n",
    "    for row in images:\n",
    "        hfile.write(\"{\")\n",
    "        row_str = \", \".join([str(elem) for elem in row])\n",
    "        hfile.write(row_str)\n",
    "        hfile.write(\"},\\n\")\n",
    "    hfile.write(\"};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_cpu]",
   "language": "python",
   "name": "conda-env-tensorflow_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
